{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "os.chdir(\"../../../../\")\n",
    "from utilities.fits import fit\n",
    "from datasets.dataset_loader import dataset_load\n",
    "from utilities import plot, gmm, errors, predict, preprocess\n",
    "from utilities.recalibration_conformal import *\n",
    "from models import seq2point_gaussian\n",
    "import time as time\n",
    "import scipy.stats as st\n",
    "# from mapie.metrics import regression_coverage_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from tueplots import bundles\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from numpyro.optim import Adam\n",
    "from numpyro.infer.autoguide import AutoDiagonalNormal\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_reductions --xla_gpu_autotune_level=2\"\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seq2point model using Flax\n",
    "class Seq2Point(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, X, deterministic):\n",
    "        X = nn.Conv(30, kernel_size=(10,))(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Conv(30, kernel_size=(8,))(X)\n",
    "        X = nn.relu(X)        \n",
    "        X = nn.Conv(40, kernel_size=(6,))(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        X = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        X = X.reshape((X.shape[0], -1))\n",
    "        X = nn.Dense(1024)(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        mean = nn.Dense(1)(X)\n",
    "        sigma = nn.softplus(nn.Dense(1)(X))\n",
    "        return mean, sigma\n",
    "\n",
    "# Define the model for NumPyro\n",
    "def model(x, y=None):\n",
    "    module = Seq2Point()\n",
    "    params = {\n",
    "        \"params\": {\n",
    "            \"Conv_0\": {\"kernel\": numpyro.sample(\"Conv_0_kernel\", dist.Normal(0, 1).expand([10, 1, 30])),\n",
    "                       \"bias\": numpyro.sample(\"Conv_0_bias\", dist.Normal(0, 1).expand([30]))},\n",
    "            \"Conv_1\": {\"kernel\": numpyro.sample(\"Conv_1_kernel\", dist.Normal(0, 1).expand([8, 30, 30])),\n",
    "                       \"bias\": numpyro.sample(\"Conv_1_bias\", dist.Normal(0, 1).expand([30]))},\n",
    "            \"Conv_2\": {\"kernel\": numpyro.sample(\"Conv_2_kernel\", dist.Normal(0, 1).expand([6, 30, 40])),\n",
    "                       \"bias\": numpyro.sample(\"Conv_2_bias\", dist.Normal(0, 1).expand([40]))},\n",
    "            \"Conv_3\": {\"kernel\": numpyro.sample(\"Conv_3_kernel\", dist.Normal(0, 1).expand([5, 40, 50])),\n",
    "                       \"bias\": numpyro.sample(\"Conv_3_bias\", dist.Normal(0, 1).expand([50]))},\n",
    "            \"Conv_4\": {\"kernel\": numpyro.sample(\"Conv_4_kernel\", dist.Normal(0, 1).expand([5, 50, 50])),\n",
    "                       \"bias\": numpyro.sample(\"Conv_4_bias\", dist.Normal(0, 1).expand([50]))},\n",
    "            \"Dense_0\": {\"kernel\": numpyro.sample(\"Dense_0_kernel\", dist.Normal(0, 1).expand([4950, 1024])),  # Adjusted input size\n",
    "                        \"bias\": numpyro.sample(\"Dense_0_bias\", dist.Normal(0, 1).expand([1024]))},\n",
    "            \"Dense_1\": {\"kernel\": numpyro.sample(\"Dense_1_kernel\", dist.Normal(0, 1).expand([1024, 1])),\n",
    "                        \"bias\": numpyro.sample(\"Dense_1_bias\", dist.Normal(0, 1).expand([1]))},\n",
    "            \"Dense_2\": {\"kernel\": numpyro.sample(\"Dense_2_kernel\", dist.Normal(0, 1).expand([1024, 1])),\n",
    "                        \"bias\": numpyro.sample(\"Dense_2_bias\", dist.Normal(0, 1).expand([1]))}\n",
    "        }\n",
    "    }\n",
    "    rng_key = random.PRNGKey(0)\n",
    "    mean, sigma = module.apply(params, x, deterministic=True, rngs={\"dropout\": rng_key})\n",
    "    mean = numpyro.deterministic(\"mu\", mean)\n",
    "    sigma = numpyro.deterministic(\"sigma\", nn.softplus(sigma))\n",
    "    # print(sigma)\n",
    "    with numpyro.plate(\"data\", x.shape[0]):\n",
    "        obs = numpyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Implementation for NILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {\n",
    "    2: {\n",
    "        'start_time': \"2013-07-01\",\n",
    "        'end_time': \"2013-07-31\"\n",
    "    },\n",
    "    5: {\n",
    "        'start_time': \"2014-07-01\",\n",
    "        'end_time': \"2014-07-31\"\n",
    "    }\n",
    "}\n",
    "test = {\n",
    "    1: {\n",
    "        'start_time': \"2014-07-01\",\n",
    "        'end_time': \"2014-07-31\"\n",
    "    }\n",
    "}\n",
    "appliances = [\"fridge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = dataset_load(appliances, train, test, 99, split_factor=0.25)\n",
    "x_train, y_train = datas[0], datas[1]\n",
    "x_cal, y_cal = datas[2], datas[3]\n",
    "x_test, y_test = datas[4], datas[5]\n",
    "x_test_timestamp = datas[6]\n",
    "scaler_x, scaler_y = datas[7], datas[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the guide for VI\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "# Set up the optimizer and inference procedure\n",
    "optimizer = Adam(step_size=0.1)\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# Initialize inference\n",
    "rng_key = random.PRNGKey(0)\n",
    "n_iterations = 500\n",
    "# init_state = svi.init(rng_key, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_result = svi.run(rng_key, n_iterations, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.plot(svi_result.losses)\n",
    "ax.set_title(\"ELBO loss\", fontsize=18, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test = jnp.arange(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = svi_result.params\n",
    "predictive = Predictive(model=model, guide=guide, params=params, num_samples=5)\n",
    "rng_key, rng_subkey = random.split(key=rng_key)\n",
    "test_posterior_predictive_samples = predictive(rng_subkey, x_test)\n",
    "\n",
    "test_idata_svi = az.from_dict(\n",
    "    posterior_predictive={\n",
    "        k: np.expand_dims(a=np.asarray(v), axis=0)\n",
    "        for k, v in test_posterior_predictive_samples.items()\n",
    "    },\n",
    "    coords={\"obs\": obs_test},\n",
    "    dims={\"mu\": [\"obs\"], \"sigma\": [\"obs\"]},\n",
    ")\n",
    "\n",
    "test_posterior_predictive_original_scale = {\n",
    "    var_name: xr.apply_ufunc(\n",
    "        scaler_y.inverse_transform,\n",
    "        test_idata_svi[\"posterior_predictive\"][var_name].expand_dims(\n",
    "            dim={\"_\": 1}\n",
    "        ),\n",
    "        input_core_dims=[[\"obs\", \"_\"]],\n",
    "        output_core_dims=[[\"obs\", \"_\"]],\n",
    "        vectorize=True,\n",
    "    ).squeeze(dim=\"_\")\n",
    "    for var_name in [\"mu\", \"sigma\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = test_posterior_predictive_original_scale['mu']\n",
    "test_sigma = test_posterior_predictive_original_scale['sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = test_mean.mean(axis=1)\n",
    "test_mean = np.array(test_mean)\n",
    "test_mean = test_mean.reshape(-1,1)\n",
    "test_sigma = test_sigma.mean(axis=1)\n",
    "test_sigma = np.array(test_sigma)\n",
    "test_sigma = test_sigma.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = scaler_y.inverse_transform(test_mean)\n",
    "test_sigma = scaler_y.inverse_transform(test_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE : {errors.rmse(y_test, test_mean):.4f} MAE  : {errors.mae(y_test, test_mean):.4f} NLL : {errors.NLL(test_mean,test_sigma,y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC Implementation for NILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import MCMC, NUTS, Predictive, HMC\n",
    "import arviz as az\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NUTS sampler\n",
    "nuts_kernel = NUTS(model, target_accept_prob=0.9)\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=25, num_samples=400)\n",
    "\n",
    "# Run inference\n",
    "rng_key = random.PRNGKey(0)\n",
    "mcmc.run(rng_key, x_train, y_train)\n",
    "\n",
    "# Extract the samples\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_test = jnp.arange(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_test = jnp.arange(x_test[:2000].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = svi_result.params\n",
    "predictive = Predictive(model=model, posterior_samples=samples)\n",
    "test_posterior_predictive_samples = predictive(jax.random.PRNGKey(0), x_test)\n",
    "\n",
    "test_idata_svi = az.from_dict(\n",
    "    posterior_predictive={\n",
    "        k: np.expand_dims(a=np.asarray(v), axis=0)\n",
    "        for k, v in test_posterior_predictive_samples.items()\n",
    "    },\n",
    "    coords={\"obs\": obs_test},\n",
    "    dims={\"mu\": [\"obs\"], \"sigma\": [\"obs\"]},\n",
    ")\n",
    "\n",
    "test_posterior_predictive_original_scale = {\n",
    "    var_name: xr.apply_ufunc(\n",
    "        scaler_y.inverse_transform,\n",
    "        test_idata_svi[\"posterior_predictive\"][var_name].expand_dims(\n",
    "            dim={\"_\": 1}\n",
    "        ),\n",
    "        input_core_dims=[[\"obs\", \"_\"]],\n",
    "        output_core_dims=[[\"obs\", \"_\"]],\n",
    "        vectorize=True,\n",
    "    ).squeeze(dim=\"_\")\n",
    "    for var_name in [\"mu\", \"sigma\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making predictions with the posterior samples\n",
    "# predictive = Predictive(model, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_new = jnp.random.randn(30, 1)\n",
    "# predictions = predictive(random.PRNGKey(1), x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract predictions\n",
    "# predicted_means = jnp.mean(predictions['obs'], axis=0)\n",
    "# predicted_stddevs = jnp.std(predictions['obs'], axis=0)\n",
    "\n",
    "# print(f\"Predicted mean: {predicted_means}\")\n",
    "# print(f\"Predicted stddev: {predicted_stddevs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = test_posterior_predictive_original_scale['mu']\n",
    "test_sigma = test_posterior_predictive_original_scale['sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = test_mean.mean(axis=1)\n",
    "test_mean = np.array(test_mean)\n",
    "test_mean = test_mean.reshape(-1,1)\n",
    "test_sigma = test_sigma.mean(axis=1)\n",
    "test_sigma = np.array(test_sigma)\n",
    "test_sigma = test_sigma.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mean = scaler_y.inverse_transform(test_mean)\n",
    "# test_sigma = scaler_y.inverse_transform(test_sigma)\n",
    "print(f\"RMSE : {errors.rmse(y_test, test_mean):.4f} MAE  : {errors.mae(y_test, test_mean):.4f} NLL : {errors.NLL(test_mean,test_sigma,y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
